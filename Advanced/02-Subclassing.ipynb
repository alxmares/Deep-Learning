{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtwfg6QwF9L1uouk/3E5Eo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","source":["# Making new layers and models via subclassing\n","### Juntado todo: Ejemplo de principio a fin\n","\n","Esto es lo que se ha aprendido:\n","* Una *layer* encapsula un estado (creado en *__init__()* o en *build()*) y algunos cómputos (definidos en *call()*).\n","* Las capas pueden anidarse recursivamente para crear nuevos bloques de cálculo más grandes.\n","* Las capas son agnósticas de backend siempre y cuando solo utilicen APIs de Keras. Se puede utilizar APIs nativas de backed (como jax.numpy, torch.nn o tf.nn), pero entonces la capa será solo utilizable con ese backend específico.\n","* Las capas pueden crear y rastrear pérdidas (típicamente pérdidas de regularación) a través de *add_loss()*.\n","* El contenedor externo, es decir, lo que se desea entrenar, es un Modelo. Un *Model* es como una *Layer*, pero con utilidades añadidas de entrenamiento y serialización.\n","\n","**Se implementará un Variational AutoEncoder (VAE) en un *backed-agnostic fashion*, de forma que corra o mismo en Tensorflow, JAX, y PyTorch. Se entreanará con dígitos MNIST.**\n","\n","El VAE que se creará será una subclase de *Model*, construído como una composición de cpas anidades de la subclase *Layer*. Tendrá una pérdida de regularización (KL divergence)."],"metadata":{"id":"C4eu3X8YIzQq"}},{"cell_type":"code","source":["import keras\n","import numpy as np\n","from keras import ops"],"metadata":{"id":"kzxZ74uRzeIT"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d90Ud-krzaCp"},"outputs":[],"source":["# Capa personalizada de Keras que realiza el muestre de una distribución latente.\n","class Sampling(keras.layers.Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n","\n","    # Inicializa la capa y  configura un generador de semillas para la aleatoriedad\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        self.seed_generator = keras.random.SeedGenerator(1337)\n","\n","    # Método que usa en el paso forward del modelo.\n","    # Toma como entrada 'z_mean' y 'z_log_var' que son la media\n","    # y el logaritmo de la varianza de una distribución normal.\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = ops.shape(z_mean)[0]\n","        dim = ops.shape(z_mean)[1]\n","        epsilon = keras.random.normal(shape=(batch, dim), seed=self.seed_generator)\n","        return z_mean + ops.exp(0.5 * z_log_var) * epsilon\n","        # Muestrea la distribución nromal mediante la técnica de 'reparametrización'\n","        # que es crucial para que el modelo pueda entrenarse mediante backpropagation.\n","\n","\n","# Define una capa que mapea las entradas (MNIS) a un trío de vector:\n","# 'z_mean', 'z_log_Var', y 'z'\n","class Encoder(keras.layers.Layer):\n","    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n","\n","    # Constructor que inicializa la capa y establece capas densas para proyeccciones\n","    # intermedias, cálculo de media y log-varianza.\n","    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.dense_proj = keras.layers.Dense(intermediate_dim, activation=\"relu\")\n","        self.dense_mean = keras.layers.Dense(latent_dim)\n","        self.dense_log_var = keras.layers.Dense(latent_dim)\n","        self.sampling = Sampling()\n","\n","  # Procesa las entradas a través de las capas densas y utiliza la clase 'Sampling'\n","  # para obtener el vector latente 'z'\n","    def call(self, inputs):\n","        x = self.dense_proj(inputs)\n","        z_mean = self.dense_mean(x)\n","        z_log_var = self.dense_log_var(x)\n","        z = self.sampling((z_mean, z_log_var))\n","        return z_mean, z_log_var, z\n","\n","\n","# Invierte el proceso del encoder, transformando el vector latente 'z' de nuevo\n","# a una representación legible (un dígito)\n","class Decoder(keras.layers.Layer):\n","    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n","\n","    # Constructor que inicializa la capa con capas densas\n","    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n","        super().__init__(name=name, **kwargs)\n","        self.dense_proj = keras.layers.Dense(intermediate_dim, activation=\"relu\")\n","        self.dense_output = keras.layers.Dense(original_dim, activation=\"sigmoid\")\n","\n","    # Procesa el vector latente a través de las capas densas para reconstruir lla salida\n","    def call(self, inputs):\n","        x = self.dense_proj(inputs)\n","        return self.dense_output(x)\n","\n","\n","# Combina el 'Encoder' y 'Decoder' en un modelo integral para entrenamiento y\n","# reconstrucción\n","class VariationalAutoEncoder(keras.Model):\n","    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n","\n","    # Constructor que inicializa el VAE, estableciendo las dimensiones y creando las\n","    # las isntancias de 'Encoder' y 'Decoder\n","    def __init__(\n","        self,\n","        original_dim,\n","        intermediate_dim=64,\n","        latent_dim=32,\n","        name=\"autoencoder\",\n","        **kwargs\n","    ):\n","        super().__init__(name=name, **kwargs)\n","        self.original_dim = original_dim\n","        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n","        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n","\n","    # Define el paso forward del modelo completo. Toma las entradas, las pasa através\n","    # del 'Encoder' para obtener 'z', y luego atavés del 'Decoder' para reconstruir\n","    # las entradas. Calcula la pérdida de divergencia KL, que es un componente\n","    # escencial de los VAEs, y las agrega a las pérdidas del modelo.\n","    def call(self, inputs):\n","        z_mean, z_log_var, z = self.encoder(inputs)\n","        reconstructed = self.decoder(z)\n","        # Add KL divergence regularization loss.\n","        kl_loss = -0.5 * ops.mean(\n","            z_log_var - ops.square(z_mean) - ops.exp(z_log_var) + 1\n","        )\n","        self.add_loss(kl_loss)\n","        return reconstructed"]},{"cell_type":"code","source":["(x_train, _), _ = keras.datasets.mnist.load_data()\n","x_train = x_train.reshape(60000,784).astype('float32')/255\n","\n","original_dim = 784\n","vae = VariationalAutoEncoder(784, 64, 32)\n","\n","optimizer = keras.optimizers.Adam(learning_rate = 1e-3)\n","vae.compile(optimizer, loss=keras.losses.MeanSquaredError())\n","\n","vae.fit(x_train, x_train, epochs=2, batch_size=64)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_IyUgKB50f9T","executionInfo":{"status":"ok","timestamp":1702818673287,"user_tz":360,"elapsed":12392,"user":{"displayName":"Alex Mares","userId":"08856922341118214227"}},"outputId":"ef0ff6f9-e62a-4ccf-ea4d-aeafa016b0b3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 0.0938\n","Epoch 2/2\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 0.0677\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7923d20cdff0>"]},"metadata":{},"execution_count":12}]}]}